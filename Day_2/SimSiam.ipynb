{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import lightly\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Habana Specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Habana modules from /usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/lib\n"
     ]
    }
   ],
   "source": [
    "from habana_frameworks.torch.utils.library_loader import load_habana_module\n",
    "load_habana_module()\n",
    "device = torch.device(\"hpu\")\n",
    "\n",
    "def habana():\n",
    " import habana_frameworks.torch.core as htcore\n",
    " htcore.mark_step()\n",
    "\n",
    "\n",
    "def permute_params(model, to_filters_last):\n",
    "    import habana_frameworks.torch.core as htcore\n",
    "    if htcore.is_enabled_weight_permute_pass() is True:\n",
    "        return\n",
    "    with torch.no_grad():\n",
    "        for name, param in model.named_parameters():\n",
    "            if(param.ndim == 4):\n",
    "                if to_filters_last:\n",
    "                    param.data = param.data.permute((2, 3, 1, 0))\n",
    "                else:\n",
    "                    param.data = param.data.permute((3, 2, 0, 1))  # permute RSCK to KCRS\n",
    "    habana()\n",
    "\n",
    "def permute_momentum(optimizer, to_filters_last):\n",
    "    import habana_frameworks.torch.core as htcore\n",
    "    if htcore.is_enabled_weight_permute_pass() is True:\n",
    "        return\n",
    "    # Permute the momentum buffer before using for checkpoint\n",
    "    for group in optimizer.param_groups:\n",
    "        for p in group['params']:\n",
    "            param_state = optimizer.state[p]\n",
    "            if 'momentum_buffer' in param_state:\n",
    "                buf = param_state['momentum_buffer']\n",
    "                if(buf.ndim == 4):\n",
    "                    if to_filters_last:\n",
    "                        buf = buf.permute((2,3,1,0))\n",
    "                    else:\n",
    "                        buf = buf.permute((3,2,0,1))\n",
    "                    param_state['momentum_buffer'] = buf\n",
    "    habana()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 8\n",
    "batch_size = 128\n",
    "seed = 1\n",
    "epochs = 5\n",
    "input_size = 160\n",
    "\n",
    "# dimension of the output of the prediction and projection heads\n",
    "out_dim = 2048\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# set the path to the dataset\n",
    "path_to_data = '/software/data/simsiam/PatternNet_unsup/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the augmentations for self-supervised learning\n",
    "collate_fn = lightly.data.ImageCollateFunction(\n",
    "    input_size=input_size,\n",
    "    # require invariance to flips and rotations\n",
    "    hf_prob=0.5,\n",
    "    vf_prob=0.5,\n",
    "    rr_prob=0.5,\n",
    "    # satellite images are all taken from the same height\n",
    "    # so we use only slight random cropping\n",
    "    min_scale=0.5,\n",
    "    # use a weak color jitter for invariance w.r.t small color changes\n",
    "    cj_prob=0.2,\n",
    "    cj_bright=0.1,\n",
    "    cj_contrast=0.1,\n",
    "    cj_hue=0.1,\n",
    "    cj_sat=0.1,\n",
    ")\n",
    "\n",
    "# create a lightly dataset for training, since the augmentations are handled\n",
    "# by the collate function, there is no need to apply additional ones here\n",
    "dataset_train_simsiam = lightly.data.LightlyDataset(\n",
    "    input_dir=path_to_data\n",
    ")\n",
    "\n",
    "# create a dataloader for training\n",
    "dataloader_train_simsiam = torch.utils.data.DataLoader(\n",
    "    dataset_train_simsiam,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SimSiam Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimSiam(nn.Module):\n",
    "    \"\"\"\n",
    "    Build a SimSiam model.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_encoder, dim=2048, pred_dim=256):\n",
    "        \"\"\"\n",
    "        dim: feature dimension (default: 2048)\n",
    "        pred_dim: hidden dimension of the predictor (default: 512)\n",
    "        \"\"\"\n",
    "        super(SimSiam, self).__init__()\n",
    "\n",
    "        # create the encoder\n",
    "        # num_classes is the output fc dimension, zero-initialize last BNs\n",
    "        self.encoder = base_encoder\n",
    "\n",
    "        # build a 3-layer projector\n",
    "        self.proj = nn.Sequential(nn.Linear(dim, dim, bias=False),\n",
    "                                        nn.Unflatten(1, torch.Size([dim, 1, 1])),nn.BatchNorm2d(dim),nn.Flatten(),\n",
    "                                        nn.ReLU(inplace=True), # first layer\n",
    "                                        nn.Linear(dim, dim, bias=False),\n",
    "                                        nn.Unflatten(1, torch.Size([dim, 1, 1])),nn.BatchNorm2d(dim),nn.Flatten(),\n",
    "                                        nn.ReLU(inplace=True), # second layer\n",
    "                                        nn.Linear(dim, dim, bias=False),\n",
    "                                        nn.Unflatten(1, torch.Size([dim, 1, 1])),nn.BatchNorm2d(dim),nn.Flatten(),\n",
    "                                        ) # output layer\n",
    "\n",
    "        # build a 2-layer predictor\n",
    "        self.predictor = nn.Sequential(nn.Linear(dim, pred_dim, bias=False),\n",
    "                                        nn.Unflatten(1, torch.Size([pred_dim, 1, 1])),nn.BatchNorm2d(pred_dim),nn.Flatten(),\n",
    "                                        nn.ReLU(inplace=True), # hidden layer\n",
    "                                        nn.Linear(pred_dim, dim)) # output layer\n",
    "\n",
    "    def forward(self, x1):\n",
    "        x1 = self.encoder(x1).flatten(start_dim=1) # NxC\n",
    "        z1 = self.proj(x1)\n",
    "        p1 = self.predictor(z1) # NxC\n",
    "        return  z1.detach(), p1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# we use a pretrained resnet for this tutorial to speed\n",
    "# up training time but you can also train one from scratch\n",
    "resnet = torchvision.models.resnet50()\n",
    "backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
    "model = SimSiam(backbone)\n",
    "model.to(device);\n",
    "\n",
    "## Habana\n",
    "permute_params(model, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SimSiam uses a symmetric negative cosine similarity loss\n",
    "criterion =  nn.CosineSimilarity()\n",
    "\n",
    "\n",
    "# scale the learning rate\n",
    "lr = 0.05 * batch_size / 256\n",
    "\n",
    "# use SGD with momentum and weight decay\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=lr,\n",
    "    momentum=0.9,\n",
    "    weight_decay=5e-4\n",
    ")\n",
    "\n",
    "## Habana\n",
    "permute_momentum(optimizer, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 237/237 [04:46<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch   0] Loss = -0.85 | Collapse Level: 0.21 / 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 237/237 [04:34<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch   1] Loss = -0.91 | Collapse Level: 0.26 / 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 237/237 [04:38<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch   2] Loss = -0.92 | Collapse Level: 0.22 / 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 237/237 [04:41<00:00,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch   3] Loss = -0.93 | Collapse Level: 0.19 / 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 237/237 [04:40<00:00,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch   4] Loss = -0.93 | Collapse Level: 0.19 / 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "avg_loss = 0.\n",
    "avg_output_std = 0.\n",
    "for e in range(epochs):\n",
    "    for (x0, x1), _, _ in tqdm(dataloader_train_simsiam):\n",
    "        x0 = x0.to(device)\n",
    "        x1 = x1.to(device)\n",
    "\n",
    "        # run the model on both transforms of the images\n",
    "        # we get projections (z0 and z1) and\n",
    "        # predictions (p0 and p1) as output\n",
    "        z0, p0 = model(x0)\n",
    "        z1, p1 = model(x1)\n",
    "\n",
    "        # apply the symmetric negative cosine similarity\n",
    "        # and run backpropagation\n",
    "        loss = 0.5 * -1*(criterion(z0, p1).mean() + criterion(z1, p0).mean())\n",
    "        loss.backward()\n",
    "        habana()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # calculate the per-dimension standard deviation of the outputs\n",
    "        # we can use this later to check whether the embeddings are collapsing\n",
    "        output = p0.detach().cpu()\n",
    "        output = torch.nn.functional.normalize(output, dim=1)\n",
    "        output_std = torch.std(output, 0)\n",
    "        output_std = output_std.mean()\n",
    "\n",
    "        # use moving averages to track the loss and standard deviation\n",
    "        w = 0.9\n",
    "        avg_loss = w * avg_loss + (1 - w) * loss.item()\n",
    "        avg_output_std = w * avg_output_std + (1 - w) * output_std.item()\n",
    "\n",
    "    # the level of collapse is large if the standard deviation of the l2\n",
    "    # normalized output is much smaller than 1 / sqrt(dim)\n",
    "    collapse_level = max(0., 1 - math.sqrt(out_dim) * avg_output_std)\n",
    "\n",
    "    # print intermediate results\n",
    "    print(f'[Epoch {e:3d}] '\n",
    "        f'Loss = {avg_loss:.2f} | '\n",
    "        f'Collapse Level: {collapse_level:.2f} / 1.00')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:38<00:00,  6.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenating the embeddings and converting to numpy\n",
      "Saving embeddings...\n",
      "Saving filenames...\n"
     ]
    }
   ],
   "source": [
    "# Create Embeddings:\n",
    "# create a torchvision transformation for embedding the dataset after training\n",
    "# here, we resize the images to match the input size during training and apply\n",
    "# a normalization of the color channel based on statistics from imagenet\n",
    "test_transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((input_size, input_size)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "# create a lightly dataset for embedding\n",
    "dataset_test = lightly.data.LightlyDataset(\n",
    "    input_dir=path_to_data,\n",
    "    transform=test_transforms\n",
    ")\n",
    "\n",
    "# create a dataloader for embedding\n",
    "dataloader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "\n",
    "embeddings = []\n",
    "filenames = []\n",
    "\n",
    "# disable gradients for faster calculations\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, (x, _, fnames) in enumerate(tqdm(dataloader_test)):\n",
    "        # move the images to the cpu\n",
    "        x = x.to(device)\n",
    "        # embed the images with the pre-trained backbone\n",
    "        y = model.encoder(x).flatten(start_dim=1)\n",
    "        # store the embeddings and filenames in lists\n",
    "        embeddings.append(y)\n",
    "        filenames = filenames + list(fnames)\n",
    "\n",
    "print(\"Concatenating the embeddings and converting to numpy\")\n",
    "embeddings = torch.cat(embeddings, dim=0)\n",
    "embeddings = embeddings.detach().cpu()\n",
    "\n",
    "\n",
    "print(\"Saving embeddings...\")\n",
    "np.save('output/embeddings', embeddings.numpy())\n",
    "\n",
    "print(\"Saving filenames...\")\n",
    "with open(\"output/filenames.txt\", \"w\") as file:\n",
    "    file.write(str(filenames))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
